# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.6
#   kernelspec:
#     display_name: venv-somalia-gcp
#     language: python
#     name: venv-somalia-gcp
# ---

# %% [markdown]
# # Model outputs
#
# <div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
# Before running this project ensure that the correct kernel is selected (top right). The default project environment name is `venv-somalia-gcp`.
# </div>
#
# This notebook utilises `.npy` and `.csv` files in the `outputs` folder and `hdf5` files in the `models` folder for `runid's` generated by `3_model_train_notebook.py` in order to see the results of model runs. You can also get `runid's` that other colleagues generated on GCP by using the `bucket_import_notebook.py`.
# %% [markdown]
# ### Checking memory usage

# %%
import os
import psutil

# Get the process ID (PID) of the current Jupyter notebook process
current_pid = os.getpid()

# Get the process memory usage
process = psutil.Process(current_pid)
memory_info = process.memory_info()

# Convert memory usage to gigabytes
memory_usage_gb = memory_info.rss / (1024 * 1024 * 1024)

# Print the memory usage in gigabytes
print("Memory usage (gb):", memory_usage_gb)

# %% [markdown]
# ## Set-up

# %% [markdown]
# ### segmentation models framework

# %%
# %env SM_FRAMEWORK = tf.keras

# %% [markdown]
# ### Import libraries & functions

# %%
import keras
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import ipywidgets as widgets


import h5py
from pathlib import Path
from keras.metrics import MeanIoU
import tensorflow as tf
from IPython.display import display

# %%
from functions_library import get_folder_paths
from loss_functions import get_combined_loss
from multi_class_unet_model_build import jacard_coef
from model_outputs_functions import (
    calculate_metrics,
    calculate_tile_metrics,
    plot_confusion_matrix,
    create_grouped_filenames,
    generate_class_names,
    update_displayed_data,
    plot_images_for_tile,
    process_tile,
    process_json_files,
)

# %% [markdown]
# ### File directories

# %%
folder_dict = get_folder_paths()
# set model and output directories
models_dir = Path(folder_dict["models_dir"])
outputs_dir = Path(folder_dict["outputs_dir"])
json_dir = Path(folder_dict["json_dir"])

# %% [markdown]
# ### Set runid

# %%
# Set runid for outputs
runid = "qa_testing_2024-01-23_0848"


# %% [markdown]
# ## Import data

# %% [markdown]
# ### Model conditions

# %%
# set model input read depending on when model was run
old_model = True
if old_model:
    model_filename = f"{runid}.hdf5"
    model_phase = h5py.File(models_dir.joinpath(model_filename), "r")
else:
    model_filename = runid
    model_phase = models_dir.joinpath(model_filename)


# %% [markdown]
# ### History (epochs)

# %%
csv_filename = f"{runid}.csv"
history = pd.read_csv(outputs_dir.joinpath(csv_filename))

# %% [markdown]
# ### Predictions

# %%
X_test_filename = f"{runid}_xtest.npy"
y_pred_filename = f"{runid}_ypred.npy"
y_test_filename = f"{runid}_ytest.npy"
filenames_filename = f"{runid}_filenamestest.npy"

X_test = np.load(outputs_dir.joinpath(X_test_filename))
y_pred = np.load(outputs_dir.joinpath(y_pred_filename))
y_test = np.load(outputs_dir.joinpath(y_test_filename))
filenames = np.load(outputs_dir.joinpath(filenames_filename))


# %% [markdown]
# ### Set loss

# %%
# check total loss has loaded successfully
total_loss = get_combined_loss()

# %% [markdown]
# ### Set classes

# %%
# number of classes
n_classes = y_test.shape[3]
n_classes

# %%
# class names
class_names = generate_class_names(n_classes)
print(class_names)

# %% [markdown]
# ## Load model

# %%
model = keras.models.load_model(
    model_phase,
    custom_objects={
        "dice_loss_plus_1focal_loss": total_loss,
        "dice_loss_plus_focal_loss": total_loss,
        "focal_loss": total_loss[0],
        "dice_loss": total_loss[1],
        "jacard_coef": jacard_coef,
    },
)


# %% [markdown]
# ## Training and validation changes <a name="#trainingvalidationchanges"></a>

# %%
# create plot showing training and validation loss
loss = history["loss"]
val_loss = history["val_loss"]

epochs = range(1, len(history.loss) + 1)

plt.plot(epochs, history.loss, "y", label="Training loss")
plt.plot(epochs, history.val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# %%
# create plot showing the IoU over time
acc = history["jacard_coef"]
val_acc = history["val_jacard_coef"]

plt.plot(epochs, history.accuracy, "y", label="Training IoU")
plt.plot(epochs, history.val_accuracy, "r", label="Validation IoU")
plt.title("Training and validation IoU")
plt.xlabel("Epochs")
plt.ylabel("IoU")
plt.legend()
plt.show()

# %% [markdown]
# ## Mean IoU

# %%
with tf.device("/cpu:0"):
    y_pred = model.predict(X_test)
    y_pred_argmax = np.argmax(y_pred, axis=3)
    y_test_argmax = np.argmax(y_test, axis=3)

# %%
# calculating mean IoU
with tf.device("/cpu:0"):
    IOU_keras = MeanIoU(num_classes=n_classes)
    IOU_keras.update_state(y_test_argmax, y_pred_argmax)
print("Mean IoU =", IOU_keras.result().numpy())

# %% [markdown]
# ## Predications across individual validation images

# %% [markdown]
# ### Group training tiles
#
# Can use `index_number` column to identify the array number to plot below

# %%
grouped_tiles_df = create_grouped_filenames(filenames)
grouped_tiles_df

# %% [markdown]
# ### Plotting individual tiles

# %%
test_img_number = 6
test_img = X_test[test_img_number]

# mask
ground_truth = y_test_argmax[test_img_number]

test_img_input = np.expand_dims(test_img, 0)
prediction = model.predict(test_img_input)
predicted_img = np.argmax(prediction, axis=3)[0, :, :]

# BGR to RGB
test_img = test_img[:, :, :3]
test_img = test_img[:, :, ::-1]
print(filenames[test_img_number])

# %%
plt.figure(figsize=(12, 8))
plt.subplot(231)
plt.title("Testing Image")
plt.imshow(test_img)  # [:, :, :3])
plt.subplot(232)
plt.title("Testing Label")
plt.imshow(ground_truth)
plt.subplot(233)
plt.title("Prediction on test image")
plt.imshow(predicted_img)
plt.show()

# %% [markdown]
# ## Polygons

# %%
# finding number of classes
unique_classes = np.unique(predicted_img)

# %% jupyter={"outputs_hidden": true}
all_results = []

for idx, (tile, filename) in enumerate(zip(X_test, filenames)):
    result_gdf = process_tile(model, tile, unique_classes, filename, idx)
    all_results.append(result_gdf)

all_polygons_gdf = pd.concat(all_results, ignore_index=True)


# %%
grouped_polygons_gdf = all_polygons_gdf.groupby(["index_num", "filename"])
grouped_counts = grouped_polygons_gdf["type"].value_counts().unstack().fillna(0)
grouped_counts = grouped_counts.reset_index()

# %% [markdown]
# ### Number of buildings (computed and actual)

# %%
building_polygon_counts = process_json_files(json_dir, grouped_counts)
building_polygon_counts

# %% [markdown]
# ## Confusion Matrix

# %% [markdown]
# ### Whole run metrics

# %%
with tf.device("/cpu:0"):
    y_true = y_test_argmax
    y_pred = model.predict(X_test)
    y_pred_arg = np.argmax(y_pred, axis=-1)


# %%
metrics_df = calculate_metrics(y_true, y_pred_arg, class_names)
metrics_df = metrics_df.set_index("Class")
metrics_df

# %% [markdown]
# ### Confusion matrix plot

# %%
plot_confusion_matrix(y_true, y_pred_arg, class_names)


# %% [markdown]
# ### Individual tile metrics

# %%
tile_metrics_df = calculate_tile_metrics(y_pred, y_test_argmax, class_names, filenames)
tile_metrics_df = tile_metrics_df.set_index("tile")
# tile_metrics_df.to_csv(str(outputs_dir) + "/" + runid + "_tile_metrics.csv")
tile_metrics_df

# %% [markdown]
# ## Quality Assuring tiles

# %%
# unique 'tile_name' values
unique_tile_names = grouped_tiles_df["tile_name"].unique()

# create a dropdown widget to select 'tile_name'
dropdown_tile_names = widgets.Dropdown(
    options=unique_tile_names, description="select tile name:"
)

display(dropdown_tile_names)

# %% [markdown]
# ### Individual tile metrics
#
# Things to look for -
# * differences between tiles
# * deviation from overall metrics

# %%
update_displayed_data(tile_metrics_df, dropdown_tile_names.value)

# %% [markdown]
# ### Individual tile plots
#
# Things to look for -
# * mix ups between tents/tiles
# * confusion with background
# * areas with good or bad predictions

# %%
plot_images_for_tile(
    model, X_test, y_test_argmax, grouped_tiles_df, dropdown_tile_names.value
)

# %% [markdown]
# ### Building counts

# %%
selected_tile_name = dropdown_tile_names.value
selected_polygons = building_polygon_counts[
    building_polygon_counts["filename"] == selected_tile_name
]
selected_polygons

# %% [markdown]
# ### Georeferencing
#
# Won't currently work without first combining training and validation masks

# %%
# mask_dir = Path(folder_dict["training_mask_dir"])
# geojson_path = mask_dir / f"{filename}.geojson"

# %%
# geojson_data = gpd.read_file(geojson_path)

# %%
# crs = geojson_data.crs
# combined_gdf.crs = crs

# %%
# merged_df = gpd.sjoin(combined_gdf, geojson_data, how="left", predicate="intersects")
