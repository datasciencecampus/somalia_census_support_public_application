# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: venv-somalia-gcp
#     language: python
#     name: venv-somalia-gcp
# ---

# %% [markdown]
# # Model outputs
#
# <div style="padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;">
# Before running this project ensure that the correct kernel is selected (top right). The default project environment name is `venv-somalia-gcp`.
# </div>
#
# #### Purpose
# This notebook processes `.npy` and `.csv` files in the `outputs` folder and `hdf5` files in the `models` folder for `runid's` generated by `3_model_train_notebook.py` in order to see the results of model runs and breakdown individual tile metrics.
#
# #### Things to note
# * Can copy in `run_id` and select `run all`
# * Check kernel
# * Run final cell to clear variables and outputs
# * Make sure you have imported the model and output files from `bucket_import_notebook.py`
# * `pre_modelling` notebook should be run - even if you aren't doing a model run - to update `.json` files
#
#
# ## Contents
#
#
# 1. ##### [Set-up](#setup)
# 1. ##### [Set run ID](#runID)
# 1. ##### [Import data](#importdata)
# 1. ##### [Load model](#loadmodel)
# 1. ##### [Training and validation changes](#trainingvalidationchanges)
# 1. ##### [Mean IOU](#meaniou)
# 1. ##### [Predictions validation images](#predictionsvalidationimages)
# 1. ##### [Polygons](#polygons)
# 1. ##### [Confusion matrix](#confusionmatrix)
# 1. ##### [Quality assuring tiles](#qatiles)
# 1. ##### [Building best fit](#bestfit)
# %% [markdown]
# ### Checking memory usage (GPU only)

# %%
import os
import psutil

# Get the process ID (PID) of the current Jupyter notebook process
current_pid = os.getpid()

# Get the process memory usage
process = psutil.Process(current_pid)
memory_info = process.memory_info()

# Convert memory usage to gigabytes
memory_usage_gb = memory_info.rss / (1024 * 1024 * 1024)

# Print the memory usage in gigabytes
print("Memory usage (gb):", memory_usage_gb)

# %% [markdown]
# ## Set-up <a name="setup"></a>

# %% [markdown]
# ### segmentation models framework

# %%
# %env SM_FRAMEWORK = tf.keras

# %% [markdown]
# ### Import libraries & functions

# %%
import keras
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import ipywidgets as widgets


import h5py
from pathlib import Path
from keras.metrics import MeanIoU
import tensorflow as tf
from IPython.display import display

# %%
from functions_library import get_folder_paths
from loss_functions import get_combined_loss
from multi_class_unet_model_build import jacard_coef
from model_outputs_functions import (
    calculate_metrics,
    calculate_tile_metrics,
    plot_confusion_matrix,
    create_grouped_filenames,
    generate_class_names,
    update_displayed_data,
    plot_images_for_tile,
    process_tile,
    process_json_files,
    building_stats,
)

# %% [markdown]
# ### File directories

# %%
folder_dict = get_folder_paths()

# set model and output directories
models_dir = Path(folder_dict["models_dir"])
outputs_dir = Path(folder_dict["outputs_dir"])
json_dir = Path(folder_dict["json_dir"])

footprints_dir = Path(folder_dict["footprints_dir"])

# %% [markdown]
# ## Set runid <a name="runID"></a>

# %%
# Set runid for outputs
runid = "qa_testing_2024-02-20_1247"


# %% [markdown]
# ## Import data <a name="importdata"></a>

# %% [markdown]
# ### Model conditions

# %%
# set model input read depending on when model was run
model_folder = True
if model_folder:
    model_filename = f"{runid}.hdf5"
    model_phase = h5py.File(models_dir.joinpath(model_filename), "r")
else:
    model_filename = runid
    model_phase = models_dir.joinpath(model_filename)


# %% [markdown]
# ### History (epochs)

# %%
csv_filename = f"{runid}.csv"
history = pd.read_csv(outputs_dir.joinpath(csv_filename))

# %% [markdown]
# ### Predictions

# %%
X_test_filename = f"{runid}_xtest.npy"
y_pred_filename = f"{runid}_ypred.npy"
y_test_filename = f"{runid}_ytest.npy"
filenames_filename = f"{runid}_filenamestest.npy"

X_test = np.load(outputs_dir.joinpath(X_test_filename))
y_pred = np.load(outputs_dir.joinpath(y_pred_filename))
y_test = np.load(outputs_dir.joinpath(y_test_filename))
filenames = np.load(outputs_dir.joinpath(filenames_filename))


# %% [markdown]
# ### Set loss

# %%
# check total loss has loaded successfully
total_loss = get_combined_loss()

# %% [markdown]
# ### Set classes

# %%
# number of classes
n_classes = y_test.shape[3]
n_classes

# %%
# class names
class_names = generate_class_names(n_classes)
print(class_names)

# %% [markdown]
# ## Load model <a name="loadmodel"></a>

# %%
model = keras.models.load_model(
    model_phase,
    custom_objects={
        "dice_loss_plus_1focal_loss": total_loss,
        "dice_loss_plus_focal_loss": total_loss,
        "focal_loss": total_loss[0],
        "dice_loss": total_loss[1],
        "jacard_coef": jacard_coef,
    },
)


# %% [markdown]
# ## Training and validation changes <a name="trainingvalidationchanges"></a>

# %%
# create plot showing training and validation loss
loss = history["loss"]
val_loss = history["val_loss"]

epochs = range(1, len(history.loss) + 1)

plt.plot(epochs, history.loss, "y", label="Training loss")
plt.plot(epochs, history.val_loss, "r", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# %%
# create plot showing the IoU over time
acc = history["jacard_coef"]
val_acc = history["val_jacard_coef"]

plt.plot(epochs, history.accuracy, "y", label="Training IoU")
plt.plot(epochs, history.val_accuracy, "r", label="Validation IoU")
plt.title("Training and validation IoU")
plt.xlabel("Epochs")
plt.ylabel("IoU")
plt.legend()
plt.show()

# %% [markdown]
# ## Mean IoU <a name="meaniou"></a>

# %%
with tf.device("/cpu:0"):
    y_pred = model.predict(X_test)
    y_pred_argmax = np.argmax(y_pred, axis=3)
    y_test_argmax = np.argmax(y_test, axis=3)

# %%
# calculating mean IoU
with tf.device("/cpu:0"):
    IOU_keras = MeanIoU(num_classes=n_classes)
    IOU_keras.update_state(y_test_argmax, y_pred_argmax)
print("Mean IoU =", IOU_keras.result().numpy())

# %% [markdown]
# ## Predications across individual validation images <a name="predictionsvalidationimages"></a>

# %% [markdown]
# ### Group training tiles
#
# Can use `index_number` column to identify the array number to plot below

# %%
grouped_tiles_df = create_grouped_filenames(filenames)
grouped_tiles_df

# %% [markdown]
# ### Plotting individual tiles

# %%
index_number = 196
test_img = X_test[index_number]

# mask
ground_truth = y_test_argmax[index_number]

test_img_input = np.expand_dims(test_img, 0)
prediction = model.predict(test_img_input)
predicted_img = np.argmax(prediction, axis=3)[0, :, :]

# BGR to RGB
test_img = test_img[:, :, :3]
test_img = test_img[:, :, ::-1]
print(filenames[index_number])

# %%
plt.figure(figsize=(12, 8))
plt.subplot(231)
plt.title("Testing Image")
plt.imshow(test_img)  # [:, :, :3])
plt.subplot(232)
plt.title("Testing Label")
plt.imshow(ground_truth)
plt.subplot(233)
plt.title("Prediction on test image")
plt.imshow(predicted_img)
plt.show()

# %% [markdown]
# ## Polygons <a name="polygons"></a>

# %%
# finding number of classes
unique_classes = np.unique(predicted_img)

# %% jupyter={"outputs_hidden": true}
all_results = []

for idx, (tile, filename) in enumerate(zip(X_test, filenames)):
    result_gdf = process_tile(model, tile, unique_classes, filename, idx)
    all_results.append(result_gdf)

all_polygons_gdf = pd.concat(all_results, ignore_index=True)


# %%
grouped_polygons_gdf = all_polygons_gdf.groupby(["index_num", "filename"])
grouped_counts = grouped_polygons_gdf["type"].value_counts().unstack().fillna(0)
grouped_counts = grouped_counts.reset_index()

# %% [markdown]
# ### Number of buildings (computed and actual)

# %%
building_polygon_counts = process_json_files(json_dir, grouped_counts)
building_polygon_stats = building_stats(building_polygon_counts)
building_polygon_stats

# %%
# saving building polygons as csv
building_polygon_stats.to_csv(
    str(outputs_dir) + "/" + runid + "_building_polygon_stats.csv"
)

# %% [markdown]
# ## Confusion Matrix <a name="confusionmatrix"></a>

# %% [markdown]
# ### Whole run metrics

# %%
with tf.device("/cpu:0"):
    y_true = y_test_argmax
    y_pred = model.predict(X_test)
    y_pred_arg = np.argmax(y_pred, axis=-1)


# %%
metrics_df = calculate_metrics(y_true, y_pred_arg, class_names)
metrics_df = metrics_df.set_index("Class")
metrics_df

# %% [markdown]
# ### Confusion matrix plot

# %%
plot_confusion_matrix(y_true, y_pred_arg, class_names)


# %% [markdown]
# ### Individual tile metrics

# %%
tile_metrics_df = calculate_tile_metrics(y_pred, y_test_argmax, class_names, filenames)
tile_metrics_df = tile_metrics_df.set_index("tile")

# %% [markdown]
# ## Quality Assuring tiles <a name="qatiles"></a>

# %%
# unique 'tile_name' values
unique_tile_names = grouped_tiles_df["tile_name"].unique()

# create a dropdown widget to select 'tile_name'
dropdown_tile_names = widgets.Dropdown(
    options=unique_tile_names, description="select tile name:"
)

display(dropdown_tile_names)

# %% [markdown]
# ### Individual tile metrics
#
# Things to look for -
# * differences between tiles
# * deviation from overall metrics

# %%
update_displayed_data(tile_metrics_df, dropdown_tile_names.value)

# %% [markdown]
# ### Individual tile plots
#
# Things to look for -
# * mix ups between tents/tiles
# * confusion with background
# * areas with good or bad predictions

# %%
plot_images_for_tile(
    model, X_test, y_test_argmax, grouped_tiles_df, dropdown_tile_names.value
)

# %% [markdown]
# ### Building counts

# %%
selected_tile_name = dropdown_tile_names.value
selected_polygons = building_polygon_stats[
    building_polygon_stats["filename"] == selected_tile_name
]
selected_polygons

# %% [markdown]
# ## Best fit <a name="bestfit"></a>

# %% [markdown]
# ### Building best fit

# %%
building_rank_indices = building_polygon_stats.groupby("filename")[
    "building_rank"
].idxmin()
building_best_df = building_polygon_stats.loc[building_rank_indices]

# %% [markdown]
# ### Tent best fit

# %%
tent_rank_indices = building_polygon_stats.groupby("filename")["tent_rank"].idxmin()
tent_best_df = building_polygon_stats.loc[tent_rank_indices]

# %% [markdown]
# ### Plot best fit

# %%
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

axes[0].hist(
    tent_best_df["accuracy_percentage_tent"], bins=10, edgecolor="black", color="green"
)
axes[0].set_title("tent")
axes[0].set_xlabel("accuracy percentage")
axes[0].set_ylabel("count")
axes[1].hist(
    building_best_df["accuracy_percentage_building"],
    bins=10,
    edgecolor="black",
    color="purple",
)
axes[1].set_title("building")
plt.tight_layout()

plt.show()

# %%
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

tent_best_df.boxplot(column="accuracy_percentage_tent", by="area", ax=axes[0])
axes[0].set_title("tent")
axes[0].set_xlabel("")
axes[0].set_ylabel("")
axes[0].grid(False)
axes[0].set_ylim(-10)

building_best_df.boxplot(column="accuracy_percentage_building", by="area", ax=axes[1])
axes[1].set_title("building")
axes[1].set_xlabel("")
axes[1].set_ylabel("")
axes[1].grid(False)
axes[1].set_ylim(-10)

plt.tight_layout()
plt.show()

# %% [markdown]
# ## Georeferencing <a name="georeference"></a>
#
# - Need to combine training and validation geojsons (think this is done in notebook #5)
# - Currently just exporting one shapefile as a test but we'll need a better solution for all outputs

# %% jupyter={"outputs_hidden": true}
test_index_num = 196
filtered_gdf = all_polygons_gdf[all_polygons_gdf["index_num"] == test_index_num]
filtered_gdf

# %%
import geopandas as gpd

filename = "training_data_baidoa_10_jo"
mask_dir = Path(folder_dict["training_mask_dir"])
geojson_path = mask_dir / f"{filename}.geojson"
geojson_data = gpd.read_file(geojson_path)

# %%
crs = geojson_data.crs
filtered_gdf.crs = crs

# %%
merged_gdf = gpd.sjoin(filtered_gdf, geojson_data, how="left", predicate="intersects")

# %%
output_file = str(footprints_dir / (filename + ".geojson"))

# %%
merged_gdf.to_file(output_file, driver="GeoJSON")
